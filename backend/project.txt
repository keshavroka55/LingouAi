# LingouAI Backend Project Analysis

This document provides a detailed analysis of the LingouAI backend project, including the technologies used, main features, and where the AI models are called.

## 1. Tools and Frameworks

The project is built with Node.js and utilizes the following tools and frameworks:

*   **Backend Framework:** Express.js (`express`) - A minimal and flexible Node.js web application framework.
*   **Database:** MongoDB with `mongoose` as the Object Data Modeling (ODM) library.
*   **Authentication:**
    *   `jsonwebtoken` (JSON Web Tokens) for creating and verifying user sessions.
    *   `bcryptjs` for securely hashing user passwords before storing them in the database.
*   **Development Tools:**
    *   `nodemon` for automatically restarting the server during development when file changes are detected.
*   **API and Network:**
    *   `axios` for making HTTP requests to external APIs.
    *   `cors` for enabling Cross-Origin Resource Sharing to allow requests from the frontend.
*   **File Handling:**
    *   `multer` for handling multipart/form-data, primarily used for file uploads (profile images, blog images).
*   **Natural Language Processing (NLP):**
    *   `natural`: A general-purpose NLP library for Node.js.
    *   `compromise`: A library for parsing and interpreting natural language.
    *   `syllable`: For counting syllables in words, used in readability calculations.
*   **Security:**
    *   `helmet`: Helps secure Express apps by setting various HTTP headers.
    *   `express-rate-limit`: Basic rate-limiting middleware for Express.
    *   `sanitize-html`: For sanitizing user-provided HTML to prevent XSS attacks.
*   **Payments:**
    *   `stripe`: For processing payments (though not fully implemented in the provided code).
*   **Utilities:**
    *   `dotenv`: To load environment variables from a `.env` file.
    *   `uuid`: For generating unique identifiers.
    *   `firebase-admin`: For integrating with Firebase services.
*   **ORM:**
    *   `prisma`: Included as a dependency, but the project primarily uses `mongoose` for database interactions.

## 2. Main Features

The backend provides the following core features:

### User Authentication (`/api/v1/auth`)

*   **User Registration:** `POST /api/v1/auth/register` - Allows new users to create an account.
*   **User Login:** `POST /api/v1/auth/login` - Authenticates users and returns a JWT.
*   **Get All Users:** `GET /api/v1/auth/getallusers` - Retrieves a list of all registered users (likely an admin-only feature).
*   **Check Credits:** `GET /api/v1/auth/credits` - Fetches the number of analysis attempts remaining for the authenticated user.

### Text Analysis (`/api/v1/analyze`)

*   **Analyze Text:** `POST /api/v1/analyze/check` - The core feature of the application. It accepts text and a set of analysis options and returns a detailed analysis. This endpoint is protected by a middleware (`checkAttempts`) that decrements the user's remaining attempts.

The analysis includes:
*   Grammar checking
*   Readability score (based on a Flesch-Kincaid-like formula)
*   Tone analysis
*   Paraphrasing suggestions
*   AI-generated content detection

### Waitlist (`/api/v1/waitlist`)

*   **Join Waitlist:** `POST /api/v1/waitlist/join` - Allows users to sign up for a waitlist by providing their email address.

### File Uploads

*   The `fileUploadService.js` sets up `multer` to handle uploads for profile and blog images, with validation for file types and size limits.

## 3. AI Model Integration

The AI/NLP models are called within the `src/controllers/analyzeController.js` file. The application leverages several external and one internal model to perform its analysis tasks.

### External Models

1.  **DeepSeek Model (via OpenRouter)**
    *   **File Path:** `src/controllers/analyzeController.js`
    *   **Functions:** `countGrammarIssues`, `getGrammarIssues`, `paraphraseText`
    *   **API Endpoint:** `https://openrouter.ai/api/v1/chat/completions`
    *   **Model Name:** `deepseek/deepseek-r1-0528-qwen3-8b:free`
    *   **Purpose:** This model is used for advanced NLP tasks, including counting and identifying grammar issues, and paraphrasing text.

2.  **PlagiarismCheck.org API**
    *   **File Path:** `src/controllers/analyzeController.js`
    *   **Function:** `checkPlagiarism`
    *   **API Endpoint:** `https://plagiarismcheck.org/api/v1/text`
    *   **Purpose:** To check the provided text for plagiarism against external sources. This feature is dependent on the `PLAGIARISM_API_KEY`.

3.  **Sapling.ai API**
    *   **File Path:** `src/controllers/analyzeController.js`
    *   **Function:** `detectAIContent`
    *   **API Endpoint:** `https://api.sapling.ai/api/v1/aidetect`
    *   **Purpose:** To detect if the text was generated by an AI. This feature is dependent on the `SAPLING_API_KEY`.

### Internal Model

1.  **Local AI Detection Model**
    *   **File Path:** `src/controllers/analyzeController.js`
    *   **Function:** `performAnalysis`
    *   **API Endpoint:** `http://localhost:8000/predict`
    *   **Purpose:** This appears to be a separate, locally-hosted microservice that provides AI detection capabilities. The backend makes a `POST` request to this endpoint with the text to be analyzed.
